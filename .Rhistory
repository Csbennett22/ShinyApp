# Use the matrix of singular vectors multiplied by singular values
X_tfidf_reduced <- svd$u %*% diag(svd$d)
return(X_tfidf_reduced)
}
# Specify directory containing text files
directory <- "C:/Users/chris/Desktop/Recipe Data"
# Call function to read text files
text_df <- read_text_files(directory)
X_tfidf <- convert_to_tfidf(text_df, 3072)
library(tm)
library(irlba)
# Function to read text files in a directory
read_text_files <- function(directory) {
# List all text files in the directory
files <- list.files(directory, pattern = "\\.txt$", full.names = TRUE)
# Initialize an empty list to store text from each file
file_texts <- list()
# Loop through each file
for (file in files) {
# Read text file
text <- readLines(file, warn = FALSE)
# Concatenate text from each file into a single string
file_text <- paste(text, collapse = " ")
# Append text from this file to the list
file_texts <- c(file_texts, file_text)
}
# Create a dataframe with each file's text as a row
df <- tibble(file_text = file_texts)
return(df)
}
# Function to convert text to TF-IDF matrix
convert_to_tfidf <- function(text_df, features) {
# Create a Corpus from the text data
corpus <- Corpus(VectorSource(text_df$file_text))
# Preprocess the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation, preserve_intra_word_dashes = TRUE)
corpus <- tm_map(corpus, stripWhitespace)
# Remove potentially empty documents
corpus <- corpus[sapply(corpus, function(x) length(unlist(x)) > 0)]
# Create a Term-Document Matrix
dtm <- DocumentTermMatrix(corpus)
# Remove sparse terms
dtm <- removeSparseTerms(dtm, 0.95)
# Convert to a matrix and calculate the TF-IDF
tdm <- as.matrix(dtm)
tfidf <- tdm * log(nDocs(dtm) / docFreq(dtm))
tfidf <- scale(tfidf, center = FALSE)  # Optional: normalize TF-IDF
# Perform SVD dimensionality reduction
# Ensure that nv does not exceed the smallest dimension of the TF-IDF matrix
nv <- min(features, nrow(tfidf) - 1, ncol(tfidf) - 1)
svd <- irlba(tfidf, nv = nv)
# Use the matrix of singular vectors multiplied by singular values
X_tfidf_reduced <- svd$u %*% diag(svd$d)
return(X_tfidf_reduced)
}
# Specify directory containing text files
directory <- "C:/Users/chris/Desktop/Recipe Data"
# Call function to read text files
text_df <- read_text_files(directory)
X_tfidf <- convert_to_tfidf(text_df, 3072)
# Function to read text files in a directory
read_text_files <- function(directory) {
# List all text files in the directory
files <- list.files(directory, pattern = "\\.txt$", full.names = TRUE)
# Initialize an empty list to store text from each file
file_texts <- list()
# Loop through each file
for (file in files) {
# Read text file
text <- readLines(file, warn = FALSE)
# Concatenate text from each file into a single string
file_text <- paste(text, collapse = " ")
# Append text from this file to the list
file_texts <- c(file_texts, file_text)
}
# Create a dataframe with each file's text as a row
df <- tibble(file_text = file_texts)
return(df)
}
# Function to convert text to TF-IDF matrix
convert_to_tfidf <- function(text_df, features) {
# Create a Corpus from the text data
corpus <- Corpus(VectorSource(text_df$file_text))
# Preprocess the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
# Remove potentially empty documents
corpus <- corpus[!sapply(corpus, function(x) length(unlist(x)) == 0)]
# Create a Term-Document Matrix
dtm <- DocumentTermMatrix(corpus)
# Convert the Term-Document Matrix to a matrix
m <- as.matrix(dtm)
# Calculate the TF-IDF
tf <- m / rowSums(m)
idf <- log(nrow(m) / colSums(m > 0))
tfidf <- tf * idf
# If there are more desired features than documents, you can't have that many features
if (ncol(tfidf) < features) {
features <- ncol(tfidf)
}
# Use Truncated SVD to reduce to the number of desired features
svd_result <- irlba(tfidf, nv = features)
# Use the d matrix from SVD as the reduced feature set
X_tfidf_reduced <- svd_result$u %*% diag(svd_result$d)
return(X_tfidf_reduced)
}
# Specify directory containing text files
directory <- "C:/Users/chris/Desktop/Recipe Data"
# Call function to read text files
text_df <- read_text_files(directory)
X_tfidf <- convert_to_tfidf(text_df, 3072)
# Function to convert text to TF-IDF matrix
convert_to_tfidf <- function(text_df, features) {
# Create a Corpus from the text data
corpus <- Corpus(VectorSource(text_df$file_text))
# Preprocess the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
# Ensure all documents have content
corpus <- corpus[sapply(corpus, function(x) length(unlist(x)) > 0)]
if (length(corpus) == 0) {
stop("All documents were dropped during preprocessing.")
}
# Create a Term-Document Matrix
dtm <- DocumentTermMatrix(corpus)
# Calculate TF-IDF
tfidf <- weightTfIdf(dtm)
# Convert to matrix
m <- as.matrix(tfidf)
# Adjust feature count if necessary
min_dimension <- min(nrow(m), ncol(m))
if (features >= min_dimension) {
features <- min_dimension - 1
cat("Features adjusted to:", features, "\n")
}
# Dimensionality reduction with SVD
svd_result <- irlba(m, nv = features)
# Return reduced dimensionality matrix
return(svd_result$u %*% diag(svd_result$d))
}
# Specify directory containing text files
directory <- "C:/Users/chris/Desktop/Recipe Data"
# Call function to read text files
text_df <- read_text_files(directory)
X_tfidf <- convert_to_tfidf(text_df, 3072)
# Save API
Sys.setenv('OPENAI_API_KEY'= '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')
# Embeddings function
GetEmbeddings <- function(input_strings) {
base_url <- 'https://api.openai.com/v1'
api_key <- Sys.getenv('OPENAI_API_KEY')
model_type <- 'text-embedding-3-large'
body <- list(input = input_strings, model = model_type)
resp <- request(base_url) %>%
req_url_path_append('embeddings') %>%
req_auth_bearer_token(token = api_key) %>%
req_headers('Content-Type' = 'application/json') %>%
req_user_agent('Christian Bennett @csbenn1') %>%
req_body_json(body) %>%
req_perform()
resp <- resp %>% resp_body_json()
embeddings <- lapply(resp$data, function(x) x[['embedding']]) %>% do.call(rbind, .)
return(embeddings)
}
# Chat function
GPTchat <- function(user_prompt = NULL,context = NULL) {
if (class(user_prompt)!='character') {stop('Error: Please enter a character string into the prompt')}
if (class(context)!='json') {stop('Error: Please enter a JSON formatted string as context')}
if (!exists('chat_history')) {
chat_history <<- list(Prompt='',Response='')
}
if (!exists('system_prompt')) {
system_prompt <<- list(list(role="system",
content="You are a helpful assistant.")
)
}
base_url <- 'https://api.openai.com/v1'
api_key <- Sys.getenv('OPENAI_API_KEY')
system_prompt <<- c(system_prompt,
list(list(role="system",content=chat_history$Response)),
list(list(role="system",content=context)), # add context, i.e. provide results of the embedding clusters
list(list(role="user",content=user_prompt))
)
body <- list(model = 'gpt-3.5-turbo',
messages = system_prompt)
req <- request(base_url)
resp <- req %>%
req_url_path_append('chat/completions') %>%
req_auth_bearer_token(token = api_key) %>%
req_headers('Content-Type' = 'application/json') %>%
req_user_agent('Christian Bennett @csbenn1') %>%
req_body_json(body) %>%
req_perform()
openai_chat_response <- resp %>% resp_body_json(simplifyVector = TRUE)
chat_history <<- c(chat_history,list(Prompt = user_prompt, Response = openai_chat_response$choices$message$content))
# return(openai_chat_response$choices$message$content)
return(chat_history[-1:-2] %>% unlist())
}
# build the database that will be used to search
#bom_embeddings <- scriptures_embeddings %>% filter(verse_title %in% (scriptures %>% filter(volume_title == 'Book of Mormon') %>% pull(verse_title)) ) %>% as.data.frame()
recipe_hnsw_index <- hnsw_build(X = X_tfidf, distance = 'cosine', M = 30, ef = 5)
# user prompt
prompt <- 'What would be the best way to make my pancakes egg-free'
# Assuming GetEmbeddings is a custom function that returns a named numeric vector of embeddings
prompt_embeddings <- data.table(recipe_title = 'prompt', embeddings = GetEmbeddings(prompt))
# Convert prompt_embeddings to matrix and remove non-numeric columns if necessary
prompt_matrix <- as.matrix(prompt_embeddings[,-1, with = FALSE])
numeric_embeddings <- numeric()
for (i in 2:ncol(prompt_embeddings)) { # starting from 2 to skip the 'recipe_title' column
numeric_embeddings <- c(numeric_embeddings, unlist(prompt_embeddings[[i]]))
}
numeric_embeddings <- sapply(prompt_embeddings[,-1, with = FALSE], unlist)
prompt_matrix <- matrix(numeric_embeddings, nrow = 1, byrow = TRUE)
# get nearest neighbors
# Perform hnsw_search with the numeric matrix of embeddings
hnsw_results_index <- hnsw_search(X = prompt_matrix, ann = recipe_hnsw_index, k = 30, ef = 5)
dim(X_tfidf)
# get nearest neighbors
# Perform hnsw_search with the numeric matrix of embeddings
hnsw_results_index <- hnsw_search(X = prompt_matrix, ann = recipe_hnsw_index, k = 30, ef = 5)
dim(X_tfidf)
dim(prompt_matrix)
# Assuming GetEmbeddings is a custom function that returns a named numeric vector of embeddings
all_embeddings <- lapply(text_df$file_text, GetEmbeddings)
# Assuming GetEmbeddings is a custom function that returns a named numeric vector of embeddings
all_embeddings <- lapply(text_df$file_text, GetEmbeddings)
rlang::last_trace()
rlang::last_trace(drop = FALSE)
prompt_embeddings <- data.table(recipe_title = 'prompt', embeddings = GetEmbeddings(prompt))
embeddings_matrix <- do.call(rbind, prompt_embeddings)
recipe_hnsw_index <- hnsw_build(X = embeddings_matrix, distance = 'cosine', M = 30, ef = 5)
# Specify directory containing text files
directory <- "C:/Users/chris/Desktop/Recipe Data"
# Call function to read text files
text_df <- read_text_files(directory)
# Function to convert text to TF-IDF matrix
convert_to_tfidf <- function(text_df, features) {
# Create a Corpus from the text data
corpus <- Corpus(VectorSource(text_df$file_text))
# Preprocess the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
# Remove potentially empty documents
corpus <- corpus[!sapply(corpus, function(x) length(unlist(x)) == 0)]
# Create a Term-Document Matrix
dtm <- DocumentTermMatrix(corpus)
# Convert the Term-Document Matrix to a matrix
m <- as.matrix(dtm)
# Calculate the TF-IDF
tf <- m / rowSums(m)
idf <- log(nrow(m) / colSums(m > 0))
tfidf <- tf * idf
# If there are more desired features than documents, you can't have that many features
if (ncol(tfidf) < features) {
features <- ncol(tfidf)
}
# Use Truncated SVD to reduce to the number of desired features
svd_result <- irlba(tfidf, nv = features)
# Use the d matrix from SVD as the reduced feature set
X_tfidf_reduced <- svd_result$u %*% diag(svd_result$d)
return(X_tfidf_reduced)
}
X_tfidf <- convert_to_tfidf(text_df, 3072)
# build the database that will be used to search
#bom_embeddings <- scriptures_embeddings %>% filter(verse_title %in% (scriptures %>% filter(volume_title == 'Book of Mormon') %>% pull(verse_title)) ) %>% as.data.frame()
recipe_hnsw_index <- hnsw_build(X = text_df, distance = 'cosine', M = 30, ef = 5)
View(text_df)
#install.packages("httr")
# setwd("C:/Users/chris/Documents/RecipeApp/ShinyApp")
library(shiny)
library(httr)
library(httr2)
library(sass)
library(markdown)
library(waiter)
library(shinyjs)
library(shinyCopy2clipboard)
library(jsonlite)
library(data.table)
library(RcppHNSW)
library(tidyverse)
library(tm)
library(irlba)
# Install shinyCopy2clipboard
if (!requireNamespace("shinyCopy2clipboard", quietly = TRUE)) {
remotes::install_github("deepanshu88/shinyCopy2clipboard")
}
# Load secrets
#source("../assets/secrets.R")
load('recipe_embeddings.Rdata')
#recipe_hnsw_index <- readRDS(file = "recipe_hnsw_index.rds")
openai_api_key <- "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"
recipe_hnsw_index <- hnsw_build(X = as.matrix(recipe_embeddings), distance = 'cosine', M = 30, ef = 5)
# Use the API key
apiKey <- openai_api_key
css <- sass(sass_file("www/chat.scss"))
jscode <- 'var container = document.getElementById("chat-container");
if (container) {
var elements = container.getElementsByClassName("user-message");
if (elements.length > 1) {
var lastElement = elements[elements.length - 1];
lastElement.scrollIntoView({
behavior: "smooth"
});
}
}'
GetEmbeddings <- function(input_strings) {
base_url <- 'https://api.openai.com/v1'
api_key <- Sys.getenv('OPENAI_API_KEY')
model_type <- 'text-embedding-3-large'
body <- list(input = input_strings,
model = model_type)
resp <- request(base_url) %>%
req_url_path_append('embeddings') %>%
req_auth_bearer_token(token = api_key) %>%
req_headers('Content-Type' = 'application/json') %>%
req_user_agent('Christian Bennett @csbenn1') %>%
req_body_json(body) %>%
req_perform()
resp <- resp %>% resp_body_json()
embeddings <- lapply(resp$data, function(x){
embedding = x[['embedding']]
}) %>% rbindlist()
return(embeddings)
}
GPTchat <- function(user_prompt = NULL,context = NULL) {
if (class(user_prompt)!='character') {stop('Error: Please enter a character string into the prompt')}
if (class(context)!='json') {stop('Error: Please enter a JSON formatted string as context')}
if (!exists('chat_history')) {
chat_history <<- list(Prompt='',Response='')
}
if (!exists('system_prompt')) {
system_prompt <<- list(list(role="system",
content="You are a helpful assistant.")
)
}
base_url <- 'https://api.openai.com/v1'
api_key <- Sys.getenv('OPENAI_API_KEY')
system_prompt <<- c(system_prompt,
list(list(role="system",content=chat_history$Response)),
list(list(role="system",content=context)), # add context, i.e. provide results of the embedding clusters
list(list(role="user",content=user_prompt))
)
body <- list(model = 'gpt-3.5-turbo',
messages = system_prompt)
req <- request(base_url)
resp <- req %>%
req_url_path_append('chat/completions') %>%
req_auth_bearer_token(token = api_key) %>%
req_headers('Content-Type' = 'application/json') %>%
req_user_agent('Christian Bennett @csbenn1') %>%
req_body_json(body) %>%
req_perform()
openai_chat_response <- resp %>% resp_body_json(simplifyVector = TRUE)
chat_history <<- c(chat_history,list(Prompt = user_prompt, Response = openai_chat_response$choices$message$content))
# return(openai_chat_response$choices$message$content)
return(chat_history[-1:-2] %>% unlist())
}
execute_at_next_input <- function(expr, session = getDefaultReactiveDomain()) {
observeEvent(once = TRUE, reactiveValuesToList(session$input), {
force(expr)
}, ignoreInit = TRUE)
}
prompt =  "What would be the best way to make my pancakes egg-free"
prompt_embeddings <- data.table(recipe_title = 'prompt', embeddings = GetEmbeddings(prompt))
prompt_matrix <- as.matrix(prompt_embeddings[,-1, with = FALSE])
hnsw_results_index <- hnsw_search(X = prompt_matrix, ann = recipe_hnsw_index, k = 60, ef = 5)
recipe_embeddings$RowNumber <- text_df$RowNumber
matched_row_numbers <- recipe_embeddings$RowNumber[hnsw_results_index$idx]
hnsw_results_recipes <- text_df %>% filter(RowNumber %in% matched_row_numbers)
prompt =  "What would be the best way to make my pancakes egg-free"
prompt_embeddings <- data.table(recipe_title = 'prompt', embeddings = GetEmbeddings(prompt))
GetEmbeddings <- function(input_strings) {
base_url <- 'https://api.openai.com/v1'
api_key <- Sys.getenv('OPENAI_API_KEY')
model_type <- 'text-embedding-3-large'
body <- list(input = input_strings,
model = model_type)
resp <- request(base_url) %>%
req_url_path_append('embeddings') %>%
req_auth_bearer_token(token = api_key) %>%
req_headers('Content-Type' = 'application/json') %>%
req_user_agent('Christian Bennett @csbenn1') %>%
req_body_json(body) %>%
req_perform()
resp <- resp %>% resp_body_json()
embeddings <- lapply(resp$data, function(x){
embedding = x[['embedding']]
}) %>% rbindlist()
return(embeddings)
}
#recipe_hnsw_index <- readRDS(file = "recipe_hnsw_index.rds")
openai_api_key <- "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"
prompt =  "What would be the best way to make my pancakes egg-free"
prompt_embeddings <- data.table(recipe_title = 'prompt', embeddings = GetEmbeddings(prompt))
install.packages("shinythemes")
#install.packages("httr")
setwd("C:/Users/chris/Documents/RecipeApp/ShinyApp")
#install.packages("httr")
#setwd("C:/Users/chris/Documents/RecipeApp/ShinyApp")
#install.packages("shinythemes")
library(shiny)
library(httr)
library(httr2)
library(sass)
library(markdown)
library(waiter)
library(shinyjs)
library(shinyCopy2clipboard)
library(jsonlite)
library(data.table)
library(RcppHNSW)
library(tidyverse)
library(tm)
library(irlba)
library(shinythemes)
library(stats)
# Install shinyCopy2clipboard
if (!requireNamespace("shinyCopy2clipboard", quietly = TRUE)) {
remotes::install_github("deepanshu88/shinyCopy2clipboard")
}
file_path <- "recipe_embeddings.rds"
if (file.exists(file_path)) {
#load('www/recipe_embeddings.Rdata')
recipe_embeddings <- readRDS("recipe_embeddings.rds")
text_df <- readRDS("text_df.rds")
} else {
print("File Not Found")
#recipe_embeddings <- read.csv("www/recipe_embeddings.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
#text_df <- read.csv("www/text_df.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
}
View(text_df)
length("For Topping Toast or Rolls")
str_length("For Topping Toast or Rolls")
View(text_df)
text_df <- text_df %>%
filter(nchar(file_text) >= 30)
View(text_df)
text_df <- text_df %>%
filter(nchar(file_text) >= 40)
# Initialize an empty list to store the embeddings from each chunk
embeddings_list <- list()
# Determine the number of rows and calculate the number of chunks needed
n_rows <- nrow(text_df)
batch_size <- 2000
n_chunks <- ceiling(n_rows / batch_size)
# Save API
Sys.setenv('OPENAI_API_KEY'= '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')
# Embeddings function
GetEmbeddings <- function(input_strings) {
base_url <- 'https://api.openai.com/v1'
api_key <- Sys.getenv('OPENAI_API_KEY')
model_type <- 'text-embedding-3-large'
body <- list(input = input_strings,
model = model_type)
resp <- request(base_url) %>%
req_url_path_append('embeddings') %>%
req_auth_bearer_token(token = api_key) %>%
req_headers('Content-Type' = 'application/json') %>%
req_user_agent('Christian Bennett @csbenn1') %>%
req_body_json(body) %>%
req_perform()
resp <- resp %>% resp_body_json()
embeddings <- lapply(resp$data, function(x){
embedding = x[['embedding']]
}) %>% rbindlist()
return(embeddings)
}
# Loop through each chunk
for (i in 1:n_chunks) {
# Calculate the row indices for the current chunk
start_index <- (i - 1) * batch_size + 1
end_index <- min(i * batch_size, n_rows)
# Extract the subset of text for the current chunk
text_subset <- text_df[start_index:end_index,]$file_text
# Get embeddings for the current chunk
current_embeddings <- GetEmbeddings(input_strings = text_subset)
# Append the embeddings for the current chunk to the list
embeddings_list[[i]] <- current_embeddings
}
# Combine all embeddings into a single object
# The specific method to combine them will depend on the structure of the embeddings
# For example, if 'current_embeddings' returns a data frame, you can use do.call(rbind, ...)
# Here, it's assumed that embeddings are in a compatible format for row-binding
recipe_embeddings <- do.call(rbind, embeddings_list)
saveRDS(recipe_embeddings, file = "recipe_embeddings.rds")
saveRDS(text_df, file = "text_df.rds")
#save data that i need:
save(recipe_embeddings, text_df, file = "recipe_embeddings.RData")
if (file.exists(file_path)) {
#load('www/recipe_embeddings.Rdata')
recipe_embeddings <- readRDS("recipe_embeddings.rds")
text_df <- readRDS("text_df.rds")
} else {
print("File Not Found")
#recipe_embeddings <- read.csv("www/recipe_embeddings.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
#text_df <- read.csv("www/text_df.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
}
file_path <- "recipe_embeddings.rds"
if (file.exists(file_path)) {
#load('www/recipe_embeddings.Rdata')
recipe_embeddings <- readRDS("recipe_embeddings.rds")
text_df <- readRDS("text_df.rds")
} else {
print("File Not Found")
#recipe_embeddings <- read.csv("www/recipe_embeddings.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
#text_df <- read.csv("www/text_df.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE, fileEncoding = "UTF-8")
}
